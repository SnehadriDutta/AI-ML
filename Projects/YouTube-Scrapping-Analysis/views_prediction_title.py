# -*- coding: utf-8 -*-
"""Youtube_Prediction_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hKpfTA4tVKvOpzKist7ybaaAImfcn9qS
"""

import pandas as pd
import requests
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

warnings.filterwarnings('ignore')

from wordcloud import WordCloud
import re
import nltk
from nltk.corpus import stopwords, words
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer
import string

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import keras
from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences

import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout, Embedding, Flatten, Bidirectional, SimpleRNN
from sklearn.metrics import mean_squared_error, r2_score

import xgboost as xg
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('words')

data = pd.read_csv('YoutubeData.csv')
data = data.sample(frac = 1)
data.head()

def remove_punctuation(text):
    regular_punct = list(string.punctuation)
    for punc in regular_punct:
        if punc in text:
            text = text.replace(punc, '')
    return text.lower().strip()

w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
lemmatizer = nltk.stem.WordNetLemmatizer()
def lemmatize_text(text):
    st = ""
    for w in w_tokenizer.tokenize(text):
        st = st + lemmatizer.lemmatize(w) + " "
    return st

emoji_pattern = re.compile("["
                               u"\U0001F600-\U0001F64F"  # emoticons
                               u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                               u"\U0001F680-\U0001F6FF"  # transport & map symbols
                               u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                               u"\U00002500-\U00002BEF"  # chinese char
                               u"\U00002702-\U000027B0"
                               u"\U00002702-\U000027B0"
                               u"\U000024C2-\U0001F251"
                               u"\U0001f926-\U0001f937"
                               u"\U00010000-\U0010ffff"
                               u"\u2640-\u2642"
                               u"\u2600-\u2B55"
                               u"\u200d"
                               u"\u23cf"
                               u"\u23e9"
                               u"\u231a"
                               u"\ufe0f"  # dingbats
                               u"\u3030"
                               "]+", flags=re.UNICODE)

regular_punct = list(string.punctuation)
stop_words = set(stopwords.words('english'))
w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
lemmatizer = nltk.stem.WordNetLemmatizer()
english_words = set(words.words())

def clean_text(text):

  #remove emoji
  sentence = emoji_pattern.sub(r'', text)

  #removing non english words and converting to lower
  sentence = ' '.join([word.lower() for word in sentence.split() if word not in (english_words)])

  #remove stop words
  sentence =  ' '.join([word for word in sentence.split() if word not in (stop_words)])

  # remove punctuation
  for punc in regular_punct:
        if punc in sentence:
            sentence = sentence.replace(punc, ' ')
  
  #lemmatize the words
  lemmatized_sentence = ""
  for w in w_tokenizer.tokenize(sentence):
        lemmatized_sentence = lemmatized_sentence + lemmatizer.lemmatize(w) + " "
  
  return lemmatized_sentence.strip()

# stop_words = set(stopwords.words('english'))
# data['clean_text']=data['Title'].apply(lambda x : remove_punctuation(x))
# data['clean_text'] = data['clean_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))
# data['clean_text'] = data['clean_text'].apply(lambda x: lemmatize_text(x))

data['clean_text']=data['Title'].apply(lambda x : clean_text(x))
data.head()

# tokenize sentences
tokenizer = Tokenizer(1000)
tokenizer.fit_on_texts(data['clean_text'])
word_index = tokenizer.word_index
# convert train dataset to sequence and pad sequences
clean_text = tokenizer.texts_to_sequences(data['clean_text']) 
clean_text = pad_sequences(clean_text, padding='pre', truncating= 'pre', maxlen=10) / 1000

Y = data['ViewsCount']
xtrain, xtest, ytrain, ytest = train_test_split(clean_text, Y, test_size = 0.2, random_state = 42)

scaler = MinMaxScaler(feature_range=(0,1))
ytrain = scaler.fit_transform(np.array(ytrain).reshape(-1,1))
ytest = scaler.transform(np.array(ytest).reshape(-1,1))

loss = tf.keras.losses.MeanSquaredError()
metric = [tf.keras.metrics.RootMeanSquaredError()]
optimizer = tf.keras.optimizers.Adam()
early_stopping = [tf.keras.callbacks.EarlyStopping(monitor = 'loss', patience = 5)]

model = Sequential()
model.add(Embedding(1000, 5, input_length=xtrain.shape[1]))
model.add(LSTM(128, return_sequences=True, input_shape= (5, 1)))
model.add(LSTM(64, return_sequences=False))
model.add(Flatten())
model.add(Dense(64, activation = 'relu'))
model.add(Dropout(0.2))
model.add(Dense(16, activation = 'relu'))
model.add(Dense(1, activation = 'relu'))
model.compile(loss = loss, metrics = metric, optimizer = optimizer)

history = model.fit(xtrain, ytrain, epochs = 300, callbacks = early_stopping)

ypred_train = model.predict(xtrain)
mse = mean_squared_error(ytrain, ypred_train.flatten())
print(f'Mean Squarred Error: {mse}')
print(f'Root Mean Squarred Error: {np.sqrt(mse)}')
print(f'Training R2 score: {r2_score(ytrain,ypred_train.flatten())}')

ypred = model.predict(xtest)
mse = mean_squared_error(ytest, ypred.flatten())
print(f'Mean Squarred Error: {mse}')
print(f'Root Mean Squarred Error: {np.sqrt(mse)}')
print(f'R2 score: {r2_score(ytest,ypred.flatten())}')

print(xtest[1])
print(scaler.inverse_transform(ytest[1].reshape(-1,1)))
print(scaler.inverse_transform(ypred[1].reshape(-1,1)))



#model.save('title_model_1.h5')

# import joblib
# joblib.dump(model,"title_model.joblib")
# joblib.dump(scaler, 'scaler.joblib')
# joblib.dump(tokenizer, 'tokenizer.joblib')