{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzijImQIYLmO"
      },
      "outputs": [],
      "source": [
        "from googleapiclient.discovery import build"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "jqDayHYwaeIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, words\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import string\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences"
      ],
      "metadata": {
        "id": "u2EgZ46safCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Input,Dense, Dropout, Flatten, LSTM, Embedding,concatenate\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import MaxPooling2D"
      ],
      "metadata": {
        "id": "Azn7QrUXajqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "id": "Dc8dbMagapE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_ChannelSubs = {}"
      ],
      "metadata": {
        "id": "LC_6aNtlYUuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting data from the YouTube API\n",
        "\n",
        "Steps to gather the Video details\n",
        "\n",
        "*   Collect the channel ids of different channels\n",
        "*   Extract the playlist id and the subscriber count of each channel\n",
        "*   Extract all the video id from the given playlist id of each channel\n",
        "*   Finally extract all the information from the video and save it in a DataFrame\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tH8lsCG1YinQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to gather the playlist ids and subscriber count of each channel\n",
        "def get_channel_data(youtube, channel_ids):\n",
        "    list_playlist_ids = []    \n",
        "    request = youtube.channels().list(\n",
        "        part='snippet,contentDetails,statistics',\n",
        "        id=','.join(channel_ids)\n",
        "    )\n",
        "    response = request.execute()\n",
        "\n",
        "    for i in range(len(response['items'])):\n",
        "      playlist_id = response['items'][i]['contentDetails']['relatedPlaylists']['uploads']\n",
        "      channel_id = response['items'][i]['id']\n",
        "      subscribers = response['items'][i]['statistics']['subscriberCount']\n",
        "      list_playlist_ids.append(playlist_id)\n",
        "      dict_ChannelSubs[channel_id] = subscribers\n",
        "\n",
        "    return list_playlist_ids"
      ],
      "metadata": {
        "id": "-5wqNlOdYa9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the video ids from each channel\n",
        "def get_playlist_videoIds(youtube, playlist_id):\n",
        "    video_ids = []\n",
        "\n",
        "    request = youtube.playlistItems().list(\n",
        "        part='contentDetails',\n",
        "        playlistId=playlist_id,\n",
        "        maxResults=50\n",
        "    )\n",
        "\n",
        "    response = request.execute()\n",
        "\n",
        "    for i in range(len(response['items'])):\n",
        "        video_ids.append(response['items'][i]['contentDetails']['videoId'])\n",
        "\n",
        "    next_page_token = response.get('nextPageToken')\n",
        "    more_pages = True\n",
        "\n",
        "    while more_pages:\n",
        "        if next_page_token is None:\n",
        "            more_pages=False\n",
        "        else:\n",
        "            request = youtube.playlistItems().list(\n",
        "                part='contentDetails',\n",
        "                playlistId=playlist_id,\n",
        "                maxResults=50,\n",
        "                pageToken=next_page_token\n",
        "            )\n",
        "\n",
        "            response = request.execute()\n",
        "\n",
        "            for i in range(len(response['items'])):\n",
        "                video_ids.append(response['items'][i]['contentDetails']['videoId'])\n",
        "\n",
        "            next_page_token = response.get('nextPageToken')\n",
        "    return video_ids"
      ],
      "metadata": {
        "id": "6S903LdqZd5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the video details and \n",
        "def get_video_details(youtube, video_ids, dict_ChannelSubs):\n",
        "\n",
        "    all_video_stats = []\n",
        "\n",
        "    for i in range(0, len(video_ids), 50):\n",
        "\n",
        "            request = youtube.videos().list(\n",
        "                part='snippet,statistics',\n",
        "                id= ','.join(video_ids[i:i+50])\n",
        "            )\n",
        "            response = request.execute()\n",
        "\n",
        "            for video in response['items']:\n",
        "                try:\n",
        "                    video_stats = dict(Id= video['id'],\n",
        "                                      Title = video['snippet']['title'],\n",
        "                                      Published_Date = video['snippet']['publishedAt'],\n",
        "                                      ThumbnailUrl= video['snippet']['thumbnails']['default']['url'],\n",
        "                                      LikesCount= video['statistics']['likeCount'],\n",
        "                                      ViewsCount= video['statistics']['viewCount'],\n",
        "                                      CommentCount= video['statistics']['commentCount'],\n",
        "                                      Subscribers= dict_ChannelSubs[video['snippet']['channelId']]\n",
        "                                      )\n",
        "                    all_video_stats.append(video_stats)\n",
        "                except:\n",
        "                      pass\n",
        "\n",
        "    return all_video_stats"
      ],
      "metadata": {
        "id": "JoFw1imWZpV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = ''\n",
        "channel_id = []"
      ],
      "metadata": {
        "id": "GaxdHZ_FZ0iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "playlist_ids = get_channel_data(youtube, channel_id)\n",
        "list_video_ids = []\n",
        "\n",
        "for id in playlist_ids:\n",
        "    video_ids = get_playlist_videoIds(youtube, id)\n",
        "    list_video_ids.extend(video_ids)\n",
        "\n",
        "video_details = get_video_details(youtube, list_video_ids, dict_ChannelSubs)"
      ],
      "metadata": {
        "id": "SYQsEz1HZ69k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame(video_details)"
      ],
      "metadata": {
        "id": "VyObR6q0Z66g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Processing"
      ],
      "metadata": {
        "id": "yh30xWaqax5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)"
      ],
      "metadata": {
        "id": "QQBRq9gwZ63I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regular_punct = list(string.punctuation)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "english_words = set(words.words())\n",
        "\n",
        "def clean_text(text):\n",
        "\n",
        "  #remove emoji\n",
        "  sentence = emoji_pattern.sub(r'', text)\n",
        "\n",
        "  #removing non english words and converting to lower\n",
        "  sentence = ' '.join([word.lower() for word in sentence.split() if word not in (english_words)])\n",
        "\n",
        "  #remove stop words\n",
        "  sentence =  ' '.join([word for word in sentence.split() if word not in (stop_words)])\n",
        "\n",
        "  # remove punctuation\n",
        "  for punc in regular_punct:\n",
        "        if punc in sentence:\n",
        "            sentence = sentence.replace(punc, ' ')\n",
        "  \n",
        "  #lemmatize the words\n",
        "  lemmatized_sentence = \"\"\n",
        "  for w in w_tokenizer.tokenize(sentence):\n",
        "        lemmatized_sentence = lemmatized_sentence + lemmatizer.lemmatize(w) + \" \"\n",
        "  \n",
        "  return lemmatized_sentence.strip()"
      ],
      "metadata": {
        "id": "4BwDPiqWZ60d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bin_subs(text):\n",
        "\n",
        "  subs = int(text)/1000000\n",
        "  bin_subs = 0\n",
        "\n",
        "  if(subs < 1):\n",
        "    bin_subs = 0\n",
        "  elif (1 < subs < 5):\n",
        "    bin_subs = 1\n",
        "  elif (5 < subs < 10):\n",
        "    bin_subs = 2\n",
        "  elif (10 < subs < 15):\n",
        "    bin_subs = 3\n",
        "  elif (15 < subs < 20):\n",
        "    bin_subs = 4\n",
        "  else:\n",
        "    bin_subs = 5\n",
        "\n",
        "  return bin_subs"
      ],
      "metadata": {
        "id": "PgG83tOCZ6xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['clean_text']=data['Title'].apply(lambda x : clean_text(x))\n",
        "data['bin_subs'] = data['Subscribers'].apply(lambda x: bin_subs(x))"
      ],
      "metadata": {
        "id": "5xrIE3bMZ6um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize sentences\n",
        "tokenizer = Tokenizer(1000)\n",
        "tokenizer.fit_on_texts(data['clean_text'])\n",
        "word_index = tokenizer.word_index\n",
        "# convert train dataset to sequence and pad sequences\n",
        "clean_text = tokenizer.texts_to_sequences(data['clean_text']) \n",
        "clean_text = pad_sequences(clean_text, padding='pre', truncating= 'pre', maxlen=10)"
      ],
      "metadata": {
        "id": "AStLuC8UZ6r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Processing"
      ],
      "metadata": {
        "id": "MDgytD_abNeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arr_images = []\n",
        "\n",
        "for i in range(data.shape[0]):\n",
        "  id = data.iloc[i]['Id']\n",
        "  viewcount = data.iloc[i]['ViewsCount']\n",
        "  img_path = f'drive//MyDrive//Thumbnails//{id}.jpg'\n",
        "\n",
        "  image = cv2.imread(img_path) \n",
        "  image = cv2.resize(image, (90,90), interpolation= cv2.INTER_LINEAR)  \n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)        \n",
        "  # Append the image and its corresponding label to the output\n",
        "  arr_images.append(image)\n",
        "\n",
        "arr_images = np.array(arr_images, dtype = 'float32') / 255.0"
      ],
      "metadata": {
        "id": "MAG5w26VZ6pN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Numercial Data Processing"
      ],
      "metadata": {
        "id": "TD2dwB-Jee97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_data = data[['LikesCount', 'CommentCount', 'bin_subs']]"
      ],
      "metadata": {
        "id": "raEJ173oeh5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building"
      ],
      "metadata": {
        "id": "sM6C1ZzpeOEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_model(input_layer, input_shape):\n",
        "\n",
        "  x = Dense(16, input_dim = input_shape, activation = 'relu')(input_layer)  \n",
        "  x = Dense(2, activation = 'relu')(x)\n",
        "\n",
        "  return x"
      ],
      "metadata": {
        "id": "gegnfiJZeNbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def title_model(input_layer, input_shape):\n",
        "\n",
        "  x = Embedding(1000, input_shape, input_length=input_shape)(input_layer)\n",
        "  x = LSTM(128, return_sequences=True, input_shape= (input_shape, 1))(x)\n",
        "  x = LSTM(64, return_sequences=False)(x)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(64, activation = 'relu')(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "  x = Dense(16, activation = 'relu')(x)\n",
        "\n",
        "  return x"
      ],
      "metadata": {
        "id": "9LMhkEK5Z6mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def thumbnail_model(input_layer, input_shape):\n",
        "\n",
        "  x = Conv2D(64, kernel_size = (2,2), strides=2, padding=\"same\", activation = 'relu', input_shape = input_shape)(input_layer)\n",
        "  x = Conv2D(64, kernel_size = (2,2), strides=2, padding=\"same\", activation  = 'relu')(x)\n",
        "  x = MaxPooling2D(pool_size = (2,2), padding=\"same\")(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "  x = Conv2D(32, kernel_size = (2,2), strides=2, padding=\"same\", activation = 'relu')(x)\n",
        "  x = MaxPooling2D(pool_size = (2,2), padding=\"same\")(x)\n",
        "  x = Conv2D(32, kernel_size = (2,2), strides=2, padding=\"same\", activation = 'relu')(x)\n",
        "  x = MaxPooling2D(pool_size = (2,2), padding=\"same\")(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(1024, activation = 'relu')(x)\n",
        "\n",
        "  return x"
      ],
      "metadata": {
        "id": "ooMW3NMcZ6j2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = data['ViewsCount']\n",
        "\n",
        "train_len = int(data.shape[0] * 0.8)\n",
        "xtrain_txt = clean_text[:train_len]\n",
        "xtest_txt = clean_text[train_len:]\n",
        "xtrain_img = arr_images[:train_len]\n",
        "xtest_img = arr_images[train_len:]\n",
        "xtrain_num = numerical_data[:train_len]\n",
        "xtest_num = numerical_data[train_len:]\n",
        "ytrain = Y[:train_len]\n",
        "ytest = Y[train_len:]"
      ],
      "metadata": {
        "id": "-4qYM6RLZ6hL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_input = Input(shape=3)\n",
        "text_input = Input(shape=10)\n",
        "image_input = Input(shape=(90,90,3))\n",
        "\n",
        "num_layers = linear_model(num_input, 3)\n",
        "text_layers = title_model(text_input, 10)\n",
        "image_layers = thumbnail_model(image_input, (90,90,3))\n",
        "\n",
        "out=concatenate([num_layers,text_layers,image_layers], axis=-1)\n",
        "output=Dense(1, activation='relu')(out)\n",
        "model = Model(inputs=[num_input,text_input, image_input], outputs=output)"
      ],
      "metadata": {
        "id": "vwwEXc-1Z6el"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.keras.losses.MeanSquaredError()\n",
        "metric = [tf.keras.metrics.RootMeanSquaredError()]\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "early_stopping = [tf.keras.callbacks.EarlyStopping(monitor = 'loss', patience = 5)]\n",
        "\n",
        "model.compile(loss = loss, metrics = metric, optimizer = optimizer)\n",
        "\n",
        "history = model.fit([xtrain_num,xtrain_txt, xtrain_img], ytrain, epochs = 300, callbacks = early_stopping)"
      ],
      "metadata": {
        "id": "gAB7FQe1Z6cH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ypred = model.predict([xtest_num, xtest_txt,xtest_img])\n",
        "mse = mean_squared_error(ytest, ypred)\n",
        "print(f'Mean Squarred Error: {mse}')\n",
        "print(f'Root Mean Squarred Error: {np.sqrt(mse)}')\n",
        "print(f'R2 score: {r2_score(ytest, ypred)}')"
      ],
      "metadata": {
        "id": "BrGxMlyzZ6aG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}