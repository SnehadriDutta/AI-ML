# -*- coding: utf-8 -*-
"""Youtube_Prdiction_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KvkTXBrRoJN5qiqN7WCAATMkXyfINXDI
"""

import pandas as pd
import cv2
import numpy as np
import re
import nltk
from nltk.corpus import stopwords, words
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer
from keras.utils import pad_sequences
import string
import joblib
import tensorflow as tf

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('words')

# thumbnail_model = tf.keras.models.load_model('./thumbnail_model1.h5')
# title_model= tf.keras.models.load_model('./title_model_1.h5')
# scaler = joblib.load('./scaler.joblib')
tokenizer = joblib.load('./tokenizer.joblib')
model = tf.keras.models.load_model('./combined_model1.h5')

emoji_pattern = re.compile("["
                               u"\U0001F600-\U0001F64F"  # emoticons
                               u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                               u"\U0001F680-\U0001F6FF"  # transport & map symbols
                               u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                               u"\U00002500-\U00002BEF"  # chinese char
                               u"\U00002702-\U000027B0"
                               u"\U00002702-\U000027B0"
                               u"\U000024C2-\U0001F251"
                               u"\U0001f926-\U0001f937"
                               u"\U00010000-\U0010ffff"
                               u"\u2640-\u2642"
                               u"\u2600-\u2B55"
                               u"\u200d"
                               u"\u23cf"
                               u"\u23e9"
                               u"\u231a"
                               u"\ufe0f"  # dingbats
                               u"\u3030"
                               "]+", flags=re.UNICODE)

regular_punct = list(string.punctuation)
stop_words = set(stopwords.words('english'))
w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
lemmatizer = nltk.stem.WordNetLemmatizer()
english_words = set(words.words())

def process_numerical(likecount, comments, subscribers):

    subs = subscribers /1000000
    bin_subs = 0

    if(subs < 1):
      bin_subs = 0
    elif (1 < subs < 5):
      bin_subs = 1
    elif (5 < subs < 10):
      bin_subs = 2
    elif (10 < subs < 15):
      bin_subs = 3
    elif (15 < subs < 20):
      bin_subs = 4
    else:
      bin_subs = 5

    array = np.array([likecount, comments, bin_subs])
    array =  np.expand_dims(array, axis=0)
    return array

def process_text(text):
    # remove emoji
    sentence = emoji_pattern.sub(r'', text)

    # removing non english words and converting to lower
    sentence = ' '.join([word.lower() for word in sentence.split() if word not in (english_words)])

    # remove stop words
    sentence = ' '.join([word for word in sentence.split() if word not in (stop_words)])

    # remove punctuation
    for punc in regular_punct:
        if punc in sentence:
            sentence = sentence.replace(punc, ' ')

    # lemmatize the words
    lemmatized_sentence = ""
    for w in w_tokenizer.tokenize(sentence):
        lemmatized_sentence = lemmatized_sentence + lemmatizer.lemmatize(w) + " "

    clean_text = tokenizer.texts_to_sequences([lemmatized_sentence.strip()])
    clean_text = pad_sequences(clean_text, padding='pre', truncating='pre', maxlen=10)

    return clean_text

def process_image(filename):

    image = cv2.imread(filename)
    image = cv2.resize(image, (90, 90), interpolation=cv2.INTER_LINEAR)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image = np.expand_dims(image, axis=0)
    return np.array(image, dtype = 'float32') / 255.0

def compute_views(likes, comments, subscribers, caption, image_path, model):

    num_vector = process_numerical(likes, comments, subscribers)
    text_vector = process_text(caption)
    image_vector = process_image(image_path)

    viewscount = model.predict([[num_vector], [text_vector], [image_vector]])[0]    
    revenue = 0.018 * viewscount

    print(f'View count: {int(viewscount)}')
    print(f'Revenue: ${int(revenue)}')
    # print(num_vector.shape)
    # print(text_vector.shape)
    # print(image_vector.shape)

prev_likes = 254100
prev_comments = 7856
subcribers = 2500000
caption = "Unboxing the new HP laptop with high performance CPU"
thumbnail = './laptop1.png'
compute_views(prev_likes, prev_comments, subcribers, caption, thumbnail, model)

prev_likes = 251000
prev_comments = 13435
subcribers = 23000000
caption = "iPhone 14 Pro Max Unboxing & First Look - The Dynamic Island MagicðŸ”¥ðŸ”¥ðŸ”¥"
thumbnail = './tj_unboxing.jpg'
compute_views(prev_likes, prev_comments, subcribers, caption, thumbnail, model)

prev_likes = 3000
prev_comments = 90
subcribers = 12000
caption = "unboxing iPhone 14 pro max (silver)ðŸ“¦+ camera samples ðŸ“¸ & accessories âœ¨"
thumbnail = './image_test_1.jpg'
compute_views(prev_likes, prev_comments, subcribers, caption, thumbnail, model)

prev_likes = 3700
prev_comments = 477
subcribers = 23000000
caption = "The Super Special Sony Folding TabletðŸ”¥ðŸ”¥ðŸ”¥"
thumbnail = './image_test_2.jpg'
compute_views(prev_likes, prev_comments, subcribers, caption, thumbnail, model)